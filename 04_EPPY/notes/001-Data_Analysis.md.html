<meta charset="utf-8">
**04EPPY - 001 - Análisis de Datos**
    <small>©2022 VIU - 04EPPY Ciencia de Datos e Inteligencia Artificial - Òscar Garibo</small>

Análisis de Datos
==============================================================

En un mundo cada vez más centralizado alrededor de las tecnologías de la información, todos los días se producen y almacenan cantidades enormes de datos. A menudo, esos datos vienen de sistemas de detección automáticos, sensores, e instrumentación científica, o son producidos por la gente de manera inconsciente cada vez que hacen una retirada de dinero del banco o hacen una compra, cuando se visitan blogs, o incluso cuando se participa en las redes sociales.

¿Pero, qué son los datos? Los datos en realidad no son información, al menos en relación a su forma. Representados como un flujo de bytes, a primera vista, es difícil entender su esencia, si no estrictamente, el número, la palabra o el tiempo que muestran. La información es realmente el resultado de un procesamiento, el cual, teniendo en cuenta un conjunto de datos determinado, extrae conclusiones que se pueden usar de variadas maneras. Este proceso de extraer información de los datos crudos se llama análisis de datos.

El propósito del análisis de datos es extraer información que no es fácilmente deducible, pero que cuando se acaba por comprender, lleva a la posibilidad de realizar estudios sobre los mecanismos de los sistemas que los han producido, permitiendo pronosticar las posibles respuestas de esos sistemas y su evolución en el tiempo.

Partiendo de un enfoque metódico simple sobre la protección de datos, el análisis de datos se ha convertido en una disciplina real, que lleva al desarrollo de metodologías reales que generan modelos. El modelo es de hecho, la traducción a forma matemática de un sistema que está bajo estudio. Una ves que existe una forma matemática o lógica que puede describir las respuestas de un sistema bajo diferentes niveles de precisión, se pueden realizar predicciones sobre su desarrollo o respuesta a determinados estímulos. Así pues, el objetivo del análisis de los datos no es el modelo, si no la calidad de su poder predictivo.

El poder predictivo de un modelo depende no solo de la calidad de las técnicas de modelado si no también de la capacidad de escoger un buen conjunto de datos sobre los que construir el proceso completo del análisis de datos. Por lo tanto, la búsqueda de datos, su extracción y su consiguiente preparación, pese a que representan actividades preliminares de un análisis, también pertenecen al análisis en sí mismo, dada su importancia en el éxito de los resultados.

Hasta ahora se ha hablado de datos, su manipulación y su procesamiento a través de procedimientos de cálculo. En paralelo a todas las etapas de procesamiento del análisis de datos, se han desarrollado varios métodos para visualizar esos datos. De hecho, para comprender los datos, tanto individualmente como en el rol que juegan en el conjunto de datos completo, no hay mejor sistema que desarrollar las técnicas de representación gráfica capaces de transformar la información, a veces, ocultas de manera implícita, en gráficos, que ayudan a entender más fácilmente su significado. A través de los años, se han desarrollado muchos sistemas para visualizar de diferentes maneras los datos.

Al final del proceso del análisis de los datos, se tendrá un modelo y un conjunto de gráficos, y en ese momento será posible predecir las respuestas del sistema bajo estudio. Tras esto, se pasa a la fase de pruebas. El modelo se probará usando otro conjunto de datos para los cuales se sabe la respuesta del sistema. Estos datos, en cambio, no se han usado para definir el modelo predictivo. Dependiendo de la capacidad del modelo para replicar las respuestas reales observadas, se tendrá un calculo del error de dicha predicción y el conocimiento sobre la validez del modelo y sus límites operativos.

Estos resultados se pueden comparar con cualquier otro modelo para averiguar si el nuevo modelo es más eficiente que los antiguos. Una vez se ha probado esto, se puede ir a la última fase del análisis de datos, el despliegue. Esto consiste en implementar los resultados producidos por el análisis, es decir, implementar las decisiones a tomar basadas en las predicciones generadas por el modelo y sus riesgos asociados.

El análisis de datos funciona muy bien para muchas actividades profesionales. Así pues, conocerlo y saber como ponerlo en práctica es relevante. Permite probar hipótesis y entender con mayor profundidad los sistemas analizados.

Dominio del conocimiento del analista de datos
==============================================================

El análisis de datos es básicamente una disciplina adecuada al estudio de los problemas que puedan ocurrir en varios campos de aplicación. Es más, el análisis de datos incluye muchas herramientas y metodologías que requieren un buen conocimiento de conceptos de informática, matemáticos y estadísticos.

Un buen analista de datos debe ser capaz de desenvolverse y actuar cómodamente en muchas áreas de disciplinas distintas. Muchas de esas disciplinas están en las bases de los métodos del análisis de datos, y se necesita bastante competencia en ellas. El conocimiento de otras disciplinas es necesario dependiendo del área de aplicación y el estudio de los proyectos particulares que se vayan a abordar, pero generalmente, una cierta experiencia en esas áreas ayudarán a comprender mejor los problemas y el tipo de datos necesario.

A menudo, respecto a los problemas importantes del análisis de datos, es necesario tener un equipo interdisciplinario de expertos que puedan contribuir de la mejor manera posible en sus respectivos campos de competencia. Sobre los problemas menores, un buen analista tiene que ser capaz de reconocer los problemas que surjan durante el análisis de datos, indagar para determinar qué disciplinas y habilidades son necesarias para solucionar esos problemas, estudiarlas, y puede que hasta preguntar a las personas con más conocimiento del sector. En resumen, el analista debe saber como buscar, no solo los datos, sino también información sobre como tratar esos datos.

Informática
--------------------------------------------------------------

El manejo de la informática es un requisito básico para cualquier analista de datos. De hecho, solo cuando se tiene un buen nivel de conocimientos y experiencia en este ámbito, se puede gestionar de manera eficiente las herramientas necesarias para el análisis de datos. Cada paso relativo al análisis de datos implica usar software de cálculo (IDL, Matlab,...) y lenguajes de programación (C++, Java, Python, R,...).

La gran cantidad de datos disponibles hoy en día, gracías a la tecnología de la información, requiere habilidades específicas para gestionarla de la mejor manera posible. De hecho, la investigación y extracción de datos requiere del conocimiento de varios formatos. Los datos están estructurados y almacenados en ficheros o bases de datos con formatos concretos. XML, JSON, o simplemente ficheros XLS o CSV, son los formatos más comunes para almacenar y coleccionar datos, y muchas aplicaciones permiten leer y gestionar los datos almacenados en ellos. Cuando hay que extraer datos contenidos en una base de datos, las cosas no son tan inmediatas, pero se necesita conocer el lenguaje SQL o usar algún programa especialmente desarrollado para la extracción de datos de una bases de datos determinada.

Es más, para algunos tipos específicos de investigación de datos, estos no están disponibles en un formato explícito, pero sí en ficheros de texto (documentos o ficheros de log) o páginas web, y muestran gráficos, medidas, número de visitantes o tablas HTML. Esto requiere una pericia específica para el análisis y potencial extracción de esos datos, el web scraping.

Así pues, el conocimiento de la informática es necesaria para saber como usar las distintas herramientas disponibles, como aplicaciones o lenguajes de programación. Estas herramientas, serán necesarias para realizar el análisis de los datos y sus visualizaciones.

Matemáticas y estadística
--------------------------------------------------------------

El análisis de datos quiere un montón de matemáticas complejas durante el tratamiento y procesamiento de los datos. Se necesita ser competente en todo esto, o al menos entender lo que se está haciendo. También se necesita cierta familiaridad con los conceptos principales de la estadística, puesto que todos los métodos que se aplican en el análisis e interpretación de los datos se basan en esos conceptos. Tal como se puede decir que la informática provee las herramientas, también se puede decir que la estadística provee los conceptos que forman la base del análisis de datos.

Esta disciplina ofrece muchas herramientas al analista, y un buen conocimiento de como mejor usarlas requiere años de experiencia. Entre las técnicas estadísticas más comunes en el análisis de datos se pueden encontrar, la probabilidad Bayesiana, análisis de regresión y análisis de grupos o clustering.

Inteligencia artificial
--------------------------------------------------------------

Una de las herramientas más avanzadas que se encuentra en el campo del análisis de datos es el aprendizaje automático o machine learning. De hecho, a pesar de la visualización de datos y técnicas como el clustering o la regresión, que deberían ayudar a encontrar información sobre el conjunto de datos, durante esta fase de investigación se suele preferir usar procedimientos especiales que están altamente especializados en buscar patrones dentro del conjunto de datos.

El aprendizaje automático es una disciplina que usa una serie de procedimientos y algoritmos que analizan los datos para reconocer patrones, agrupaciones o tendencias, y luego extrae información útil para analizar los datos de una manera automática.

Esta disciplina se está convirtiendo en una herramienta fundamental en el análisis de datos, y por tanto, un cierto conocimiento de ella, aunque sea general, es de importancia fundamental para el analista de datos.

Campos profesionales de aplicación
--------------------------------------------------------------

Otro punto importante es el dominio de competencia de los datos, su origen, biología, física, finanzas, prueba de materiales, estadísticas de población,... De hecho, pese a que los analistas suelen tener formación especializada en el campo de la estadística, también tienen que ser capaces de documentarse sobre el origen de los datos, con el propósito de entender mejor los mecanismos que han generados esos datos. Los datos no suelen ser simples números o cadenas de carácteres, son la expresión, o la medición de algún parámetro observado. Con lo que, un mejor entendimiento de donde provienen los datos puede mejorar dramáticamente su interpretación. A menudo, sin embargo, esto es muy costoso para los analistas de datos, incluso con las mejores intenciones, así que suele ser una buena práctica encontrar consultores o figuras claves a quien realizar las preguntas correctas.

Entendiendo la naturaleza de los datos
==============================================================

El objeto de estudio del análisis de datos son los datos, y por tanto, estos se convierten en la clave en todos los procesos de análisis de datos. Constituyen la materia prima que se tiene que procesar, y gracias a ese procesamiento y análisis, es posible extraer cierta información para aumentar el nivel de conocimiento del sistema bajo estudio, es decir, de donde vinieron los datos.

Cuando los datos se convierten en información
--------------------------------------------------------------

Los datos son los eventos registrados en el mundo real, cualquier cosas que se pueda medir o categorizar puede ser convertida en datos. Una vez recogidos, estos datos pueden ser estudiados y analizados, tanto para entender la naturaleza de los eventos como para realizar predicciones o al menos, tomar decisiones razonadas.

Cuando la información se convierte en conocimiento
--------------------------------------------------------------

Se puede hablar de conocimiento cuando la información se convierte en un conjunto de reglas que ayuda a entender ciertos mecanismos y por lo tanto hacer predicciones sobre la evolución de algunos eventos.

Tipos de datos
--------------------------------------------------------------

Los datos se pueden dividir en dos categorías distintas.

- Categóricos, nominales y ordinales
- Numéricos, discretos y continuos

Los datos categóricos son valores u observaciones que se pueden dividir en grupos o categorías. Hay dos tipos de valores categóricos, nominales y ordinales. Una variable nominal no tiene orden intrínseco que esté definido en su categoría. Una variable ordinal en cambio, si que tiene un orden predeterminado.

Los datos numéricos son valores u observaciones que viene de medidas. Hay dos tipos, discretos y continuos. Los valores discretos pueden ser contados y son distintos y separados entre ellos. En cambio, los continuos, son valores producidos por medidas que pueden recibir cualquier valor dentro de un rango definido.

El proceso del análisis de datos
==============================================================

El análisis de datos puede ser descrito como un proceso que consta de varios pasos a través de los cuales los datos crudos se transforman y procesan para producir visualizaciones y hacer predicciones gracias a un modelo matemático basado en los datos recogidos. Desde ese punto de vista, el análisis de datos no es más que una secuencia de pasos, cada uno de ellos juega un papel clave en los subsiguientes. Es un proceso en cadena con las siguientes fases.

- Definición del problema
- Extracción de los datos
- Preparación de los datos, limpieza
- Preparación de los datos, transformación
- Exploración y visualización de los datos
- Modelado predictivo
- Validación/test del modelo
- Despliegue, visualización e interpretación de los resultados
- Despliegue de la solución

![Figure [res/001_000]: Proceso de análisis de datos](res/001_000.png)

Definición del problema
--------------------------------------------------------------

El proceso del análisis de datos empieza realmente antes de la recolección de los datos crudos. De hecho, siempre empieza con un problema a solucionar, que tiene que estar definido.

El problema se define solo después de haberse centrado sobre el sistema que se quiere estudiar, puede ser un mecanismo, una aplicación o un proceso en general. Este estudio suele llevarse a cabo para entender mejor su manera de operar, pero en particular, ese estudio se diseñará para entender los principios de su comportamiento para poder realizar predicciones o elecciones (definidas como elecciones informadas).

El paso de definición y su correspondiente documentación del problema es importante para enfocar todo el análisis estrictamente en obtener resultados. De hecho, un estudio extenso y exhaustivo de dicho sistema a veces es complejo y no siempre se tiene suficiente información para empezar. Así pues, la definición del problema y especialmente su planificación puede determinar las pautas a seguir para todo el proyecto.

Una vez el problema ha sido definido y documentado, se progresa hacia la fase de planificación del proyecto. Esta planificación se necesita para averiguar qué profesionales y recursos son necesarios para alcanzar los requisitos para llevar a buen puerto el proyecto tan eficientemente como sea posible. Así que se consideran los problemas en el área relativos a la resolución del problema. Se buscan especialistas en varias áreas se instala el software que se va a necesitar para el trabajo. Se elije un equipo de trabajo, que suele ser interdisciplinario para poder solucionar el problema mirando a los datos desde diferentes perspectivas. Y este es uno de los factores clave que pueden llevar al éxito del análisis de datos.

Extracción
--------------------------------------------------------------

Una vez el problema está definido, el primer paso es obtener los datos para realizar el análisis. Estos datos deben elegirse con el propósito básico de construir el modelo predictivo, así que la selección de estos datos es crucial para el éxito del análisis. Los datos de muestra recogidos debe reflejar tanto como sea posible el mundo real, es decir, como responde el sistema a estímulos del mundo real. Por ejemplo, si se usan conjuntos de datos muy grandes de datos crudos y no se han recogido de manera adecuada, esto puede llevar situaciones mal balanceadas o incluso mal representadas.

La mala elección de los datos, o incluso realizar un análisis en un conjunto de datos que no representa perfectamente al sistema, puede llevar a modelos que se alejen del sistema bajo estudio.

La búsqueda y obtención de los datos a menudo necesita una forma de intuición que va más allá de la mera investigación técnica. Este proceso también requiere un entendimiento profundo de la naturaleza y forma de los datos, que solo se consigue con mucha experiencia y conocimiento en el campo de aplicación del problema.

Pese a la calidad y calidad de los datos necesarios, otro problema es usar las mejores fuentes de información. Si el entorno del estudio es un laboratorio (técnico o científico) y los datos generados son experimentales, en este caso el origen de los datos es fácilmente identificable. En este caso, los problemas solo estarán relaciones con la configuración del experimento.

Pero no es posible reproducir los sistemas en los cuales los datos se han obtenido de una manera estrictamente experimental en cada campo de aplicación. Muchos campos necesitan buscar datos del mundo que los rodea, a menudo confiando en datos experimentales externos, o incluso recogiéndolos a través de entrevistas o encuestas. En esos casos, encontrar un buen origen de datos que sea capaz de ofrecer toda la información necesaria puede ser complicado. A menudo es necesario obtener datos de varias fuentes para complementar alguna carencia, identificar posibles discrepancias y hacer al conjunto de datos lo más genérico posible.

Cuando se quieren recoger datos, un buen sitio para empezar suele ser la Web. Pero la mayoría de los datos en Internet suelen ser difíciles de capturar, de hecho, no todos esos datos suelen estar disponibles en un fichero o una base de datos, si no que pueden estar dentro de páginas HTML en muchos formatos distintos. Para este efecto, técnicas como el web scraping permite la recolección de datos a través del reconocimiento de ocurrencias específicas de etiquetas HTML dentro de las páginas web. Existe software especialmente diseñado para este propósito, y una vez se encuentra una ocurrencia, extrae los datos buscados. Cuando se completa la búsqueda, se tiene un grupo de datos listo para ser sujeto del análisis.

Preparación
--------------------------------------------------------------

Entre todos los pasos involucrados en el análisis de datos, la preparación de esos datos, aunque parezca menos problemática, requiere más recursos y tiempo para ser completada. Los datos suelen venir de diferentes fuentes, cada una de ellas con sus datos con diferentes representaciones y formatos. Así que, todos esos datos tiene estar preparados para el proceso posterior.

La preparación de los datos tiene que ver con obtener, limpiar, normalizar y transformar los datos en un conjunto de datos optimizado. Es decir, en un formato preparado que suele ser tabular y es adecuado para los métodos del análisis que han sido programados durante la fase de diseño.

Pueden surgir muchos problemas, incluyendo valores inválidos, ambiguos o ausentes, campos replicados o datos fuera de rango.

Exploración/Visualización
--------------------------------------------------------------

Explorar los datos consiste básicamente en buscar los datos en una presentación gráfica o estadística para poder encontrar patrones, conexiones y relaciones. La visualización de los datos es la mejor herramienta para sacar a la luz posibles patrones.

En los últimos tiempos, la visualización de los datos se ha desarrollado de tal manera que se ha convertido en una disciplina en sí misma. De hecho, se usan varias tecnologías exclusivamente para mostrarlos, y extraer la máxima información posible de un conjunto de datos.

La exploración de los datos es un examen preliminar de los mismos, lo cual es importante para entender el tipo de información que se ha recogido y lo que significa. En combinación con el conocimiento adquirido durante la definición del problema, esta categorización determinará que método de análisis de datos será el más adecuado para llegar a la definición del modelo.

Generalmente, esta fase, además de un estudio detallado de los gráficos a través de la visualización de los datos, puede conllevar alguna de estas actividades:

- Sintetizar los datos, en este paso los datos se reducen sin sacrificar información importante
- Agrupar los datos, clustering, se encuentran grupos unidos por atributos comunes
- Explorar las relaciones entre los distintos atributos y anomalías
- Identificar patrones y tendencias
- Construir modelos de regresión
- Construir modelos de clasificación

Otros métodos de la minería de datos, como árboles de decisión y reglas de asociación, extraen automáticamente hechos o reglas de los datos. Estas aproximaciones pueden ser usadas en paralelo con la visualización de datos para descubrir relaciones entre los datos.

Modelado predictivo
--------------------------------------------------------------

Este es un proceso usado para crear o elegir un modelo estadístico adecuado para predecir la probabilidad de un resultado.

Tras explorar los datos, se tiene toda la información necesaria para desarrollar el modelo matemático que codifica las relaciones entre los datos. Estos modelos son útiles para entender el sistema bajo estudio, y se usan para dos propósitos principales. El primero es para hacer predicciones sobre los valores de los datos producidos por el sistema, en este caso, se habla de modelos de regresión. El segundo propósito es clasificar nuevos datos, en este caso, se usan modelos de clasificación o de agrupación (clustering). De hecho, es posible dividir los modelos de acuerdo al tipo de resultado que producen.

- Modelos de clasificación, si el resultado obtenido por el modelo es categórico
- Modelos de regresión, si el resultado obtenido por el modelo es numérico
- Modelos de agrupación o clustering, si el resultado obtenido por el modelo es descriptivo

Algunos métodos simples para generar estos modelos incluyen técnicas tales como la regresión lineal, regresión logística, árboles de clasificación o regresión, y k-vecinos más próximos (k-nearest neighbors). Pero los métodos de análisis son numerosos, y cada uno tiene sus características específicas que lo hacen perfecto para algunos tipos de datos y análisis. Cada uno de esos métodos producirá un modelo específico, y su elección serña relevante para la naturaleza de lo que produzca dicho modelo.

Algunos de estos modelos darán valores que corresponden con el sistema real de acuerdo con su estructura. Explicarán algunas características del sistema bajo estudio de una manera simple y limpia. Otros modelos quizás hagan haciendo buenas predicciones, pero su estructura no será más que una caja negra con una habilidad limitada para explicar las características del sistema.

Validación del modelo
--------------------------------------------------------------

Esta fase de pruebas, es importante puesto que permite validar el modelo construido en base a los datos de arranque. Permite evaluar la validez de los datos producidos por el modelo comparándolos directamente con el sistema real. Pero esta vez, estos datos vienen del conjunto de datos originales sobre los cuales se ha establecido todo el análisis.

Generalmente, se refiere a estos datos como el conjunto de entrenamiento cuando se usan para construir el modelo, y como conjunto de validación cuando se usan para validar el modelo.

Así pues, comparando los datos producidos por el modelo contra los producidos por el sistema real, se puede evaluar el error, y usando varios conjuntos de datos de prueba, se pueden estimar los límites de la validez del modelo generado. De hecho, los valores correctamente predichos pueden ser validos solo dentro de un determinado rango, o tener diferentes niveles de coincidencia dependiendo del rango de valores que se tengan en cuenta.

Este proceso permite no solo evaluar numéricamente la efectividad del modelo, si no también compararlo contra otros modelos existentes. Hay muchas técnicas para esto, la más famosa es la validación cruzada, cross validation. Esta técnica se basa en la división del conjunto de entrenamiento en varias partes. Cada una de esas partes, será usada como el conjunto de validación y cualquier otra como el conjunto de entrenamiento. De esta manera iterativa, se puede tener un modelo perfeccionado.

Despliegue
--------------------------------------------------------------

Este es el paso final, cuyo objetivo es presentar los resultados, las conclusiones del análisis. En un entorno empresarial, el análisis se traduce en un beneficio para el cliente que lo ha encargado. En entornos técnicos o científicos, se traduce en soluciones de diseño o publicaciones científicas. De esta manera, el despliegue básicamente consiste en poner en práctica los resultados obtenidos por el análisis de los datos.

Hay muchas maneras de desplegar los resultados del análisis de datos o de la minería. Algunas veces, simplemente consiste en escribir un informe para el cliente que ha pedido el análisis. Este documento describirá de manera conceptual los resultados obtenidos del análisis, se redirigirá este documento a las personas responsables, que entonces pueden tomar decisiones basadas en eso, poniendo en práctica las conclusiones del análisis.

En la documentación final realizada por el analista, se suelen discutir en detalle algunos temas:

- Resultados del análisis
- Toma de decisiones
- Análisis de riesgos
- Medición del impacto

Cuando los resultados del proyecto incluyen la generación de modelos predictivos, estos modelos pueden ser desplegados como aplicaciones independientes o pueden ser integrados en otro software.

Análisis de datos cuantitativo y cualitativo
==============================================================

El análisis de datos está completamente centrado en los datos. Dependiendo de la naturaleza de los mismos, se pueden hacer ciertas distinciones.

Cuando los datos analizados tienen una estructura numérica o categórica, entonces se habla de análisis cuantitativo, pero cuando se trabaja con datos que se expresan a través de descripciones en lenguaje natural, se habla de análisis cualitativo.

El análisis cuantitativo está relacionado con datos con un orden lógico o que pueden ser categorizados de alguna manera. Esto lleva a la formación de estructuras dentro de los datos. El orden, categorización y estructuras ofrecen más información y permiten el procesamiento de los datos de una mánera más matemática. Esto lleva a la generación de modelos que dan predicciones cuantitativas, permitiendo al analista diseñar conclusiones más objetivas.

Em cambio, el análisis cualitativo tiene que ver con datos que generalmente no tienen una estructura, o al menos, no es evidente, y su naturaleza no es numérica ni categórica. Por ejemplo, los datos bajo estudio cualitativos pueden incluir texto escrito, datos visuales o auditivos,... Este tipo de análisis tiene que estar basado en metodologías, a menudo hechas a medida, para extraer información que generalmente llevarán a modelos capaces de hacer predicciones cualitativas. Con esto el analista puede llegar a conclusiones que puedan incluir interpretaciones subjetivas. Por otra parte, el análisis cualitativo puede explorar sistemas más complejos y obtener conclusiones que no son posibles usando una aproximación estrictamente matemática. A menudo, este tipo de análisis involucra el estudio de sistemas como los fenómenos sociales o estructuras complejas que no son fácilmente medibles.

![Figure [res/001_001]: Análisis Cuantitativos y Cualitativos](res/001_001.png)

<link rel="stylesheet" href="res/md/viu.css">
<style class="fallback">body{visibility:hidden}</style><script>markdeepOptions={tocStyle:'long'};</script>
<!-- Markdeep: --><script src="res/md/markdeep.min.js?" charset="utf-8"></script>
